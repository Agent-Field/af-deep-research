# AF Deep Research - Environment Configuration
# ================================================
# Copy this file to .env and fill in your API keys:
#   cp .env.example .env
#
# Then run:
#   docker-compose up

# ===================
# Required API Keys
# ===================

# OpenRouter API Key (for LLM inference)
# Get yours at: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-your-openrouter-api-key-here

# ===================
# Web Search API Keys
# ===================
# Configure at least ONE of the following search providers.
# The system will auto-detect which one is available.

# Jina AI (recommended - free tier available)
# Get yours at: https://jina.ai/
JINA_API_KEY=jina_your-jina-api-key-here

# Tavily (AI-native search, 1000 free/month)
# Get yours at: https://tavily.com/
# TAVILY_API_KEY=tvly-your-tavily-api-key-here

# Firecrawl (search + scraping, 500 free credits)
# Get yours at: https://firecrawl.dev/
# FIRECRAWL_API_KEY=fc-your-firecrawl-api-key-here

# Serper (Google SERP, 2500 free/month)
# Get yours at: https://serper.dev/
# SERPER_API_KEY=your-serper-api-key-here

# ===================
# Optional Settings
# ===================

# Force a specific search provider (auto-detect if not set)
# Options: jina, tavily, firecrawl, serper
# SEARCH_PROVIDER=jina

# ===================
# LLM Model Configuration
# ===================
# Default LLM model to use (via OpenRouter)
# Format: openrouter/<provider>/<model-name>
# See all models: https://openrouter.ai/models
#
# RECOMMENDED MODELS (all verified to work with complex JSON output):
#
# --- Best Value (Default) ---
# openrouter/deepseek/deepseek-chat-v3.1      # $0.15/$0.75 per 1M tokens - excellent reasoning, 128K context
# openrouter/deepseek/deepseek-v3.2           # Latest DeepSeek, similar pricing
#
# --- Premium (Highest Quality) ---
# openrouter/anthropic/claude-sonnet-4        # $3/$15 per 1M tokens - excellent for complex analysis
# openrouter/anthropic/claude-opus-4          # $15/$75 per 1M tokens - best reasoning capability
#
# --- Budget-Friendly ---
# openrouter/google/gemini-2.5-flash          # $0.30/$2.50 per 1M tokens - fast, 1M context
# openrouter/google/gemini-2.5-flash-lite     # Even cheaper, optimized for speed
#
# --- Open Source ---
# openrouter/qwen/qwen2.5-72b-instruct        # Excellent JSON/structured output
# openrouter/qwen/qwen3-235b-a22b             # Latest Qwen, very capable
# openrouter/meta-llama/llama-3.3-70b-instruct # Good general purpose
# openrouter/meta-llama/llama-3.1-405b-instruct # Largest open model
#
DEFAULT_MODEL=openrouter/deepseek/deepseek-chat-v3.1

# LLM temperature (0.0-1.0, lower = more deterministic)
TEMPERATURE=0.6

# ===================
# Local Ollama Configuration (Optional)
# ===================
# To use a local Ollama deployment instead of OpenRouter:
#   1. Set OLLAMA_BASE_URL to your Ollama server
#   2. Change DEFAULT_MODEL to use ollama/<model-name> format
#   3. OPENROUTER_API_KEY is not required when using Ollama
#
# IMPORTANT: When running via Docker, use host.docker.internal instead of localhost
# to reach Ollama running on your host machine:
#
#   OLLAMA_BASE_URL=http://host.docker.internal:11434
#   DEFAULT_MODEL=ollama/llama3.2
#
# For non-Docker (running main.py directly):
#   OLLAMA_BASE_URL=http://localhost:11434
#   DEFAULT_MODEL=ollama/llama3.2
#
# For remote Ollama server:
#   OLLAMA_BASE_URL=http://your-ollama-server:11434
#   DEFAULT_MODEL=ollama/llama3.1:70b
#
# Popular Ollama models:
#   ollama/llama3.2           # 3B params, fast
#   ollama/llama3.1:8b        # 8B params, good balance
#   ollama/llama3.1:70b       # 70B params, high quality
#   ollama/mistral            # 7B params, fast
#   ollama/mixtral            # 8x7B MoE, powerful
#   ollama/deepseek-r1:14b    # DeepSeek reasoning model
#   ollama/qwen2.5:72b        # Qwen 2.5, excellent for structured output
#
# OLLAMA_BASE_URL=http://host.docker.internal:11434
# DEFAULT_MODEL=ollama/llama3.2
